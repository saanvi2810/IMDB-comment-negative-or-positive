# -*- coding: utf-8 -*-
"""Untitled114.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MmtKNH9wzBgvoH27QCIYvqndtX1gkvoi
"""

#  The IMDB dataset contains 50,0000  reviews  from internet movie datasets.  They  are split into 25,000 reviews for training, and 25,000 for  testing. each set containinig 
# 50% negative and  50% positive reviews.

from keras.datasets  import imdb
(train_data, train_labels),(test_data, test_labels)= imdb.load_data(num_words =10000)

train_data[0]

train_labels[0]

# We will encode the integer sequence of the  reviews into binary  sequence

import numpy as np

#Creating a function to  vectorize the  sequence

def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence  in enumerate(sequences):
    results[i, sequence]=1
  return results

x_train =vectorize_sequences(train_data)
x_test=vectorize_sequences(test_data)

# Now the  sequence has been converted to the binary , vectorized format
x_train[0]

#Now we will vectorize  the labels(outputs)
y_train=np.asarray(train_labels).astype('float32')
y_test=np.asarray(test_labels).astype('float32')

# Since the data now is  vectorized, we can now feed this data into  the neural network. 
# Defining the model

from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation ='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation ='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

#  Now that the model is  defined , we need to chose  a loss function and  an  optimizer 
model.compile(optimizer='rmsprop', loss ="binary_crossentropy", metrics =['accuracy'])

# So in order to monitor during the training the accuracy of the model on  data  it has never seen before, we will create  a  validation  set by setting apart  10,000 samples from the original  training data

x_val=x_train[:10000]
partial_x_train=x_train[10000:]
y_val=y_train[:10000]
partial_y_train=y_train[10000:]

# Training the model for 20 epochs(ie  20 iterations over x_train and y_train tensors in mini batches of  512 samples.). At the same time , you will be monitoring loss 
#and accuracy on 10000 samples that  has been set apart above



history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val,y_val))

# At the end of every epochs  there is a  slight delay as  the model computes its loss  and accuracy over 10000 samples of the validation data

results = model.evaluate(x_test, y_test)

# We can now generate prediction on the new data 
predict =model.predict(x_test)

predict

# We can see the the model is confident on samples  which are close to 0.99 or 0.11 but not confident on samples which are close to 0.4, 0.6











# As we can see the model is  confident on some samples ( example , 0.99 or 0.011), however it is not confident on other samples like 0.04









